# 1 CODIFICACIÓN

### ASCII (American Standard Code for Information Interchange)

ASCII es un esquema de codificación de caracteres basado en el alfabeto inglés (alfabeto latino que consta de 26 letras). Según IANA, el nombre más apropiado para este código de caracteres es "US-ASCII" (nombre MIME recomendado).

ASCII define 128 caracteres (que van de 0 a 127 en base decimal) y utiliza un código binario de 7 bits para su representación. El código ASCII es simplemente una representación numérica en código binario de un carácter; un método para una correspondencia entre secuencias de bits y una serie de símbolos (alfanuméricos y otros). 

El código ASCII reserva los primeros 32 códigos (numerados del 0 al 31 en decimal) para caracteres de control: códigos no pensados originalmente para representar información imprimible, sino para controlar dispositivos (como impresoras) que usaban ASCII. El código 127 (los siete bits a uno), otro carácter especial, equivale a "suprimir" ("delete"). Los códigos del 33 al 126 se conocen como caracteres imprimibles, y representan letras, dígitos, signos de puntuación y varios símbolos.

![](images/01-ascii.png?raw=true)

A medida que la tecnología informática se difundió a lo largo del mundo, se desarrollaron muchas variaciones del código ASCII para facilitar la escritura de lenguas diferentes al inglés que usaran alfabetos latinos. Se pueden encontrar algunas de esas variaciones clasificadas como "ASCII Extendido". Se denomina ASCII extendido a cualquier esquema de codificación de caracteres de ocho o más bits que preserve el significado del conjunto de códigos de caracteres original ASCII de siete bits. Esto significa que todos los caracteres ASCII están codificados con un solo byte con el valor que se utiliza en ASCII para codificar ese caracter. Uno de los más comunes es el código ISO-8859-1 (Latin-1).

![](images/01-iso8859-1.png?raw=true)

### UNICODE (character set) y UTF-8 (encoding)

Unicode es un conjunto de caracteres diseñado para ser capaz de representar los caracteres de todos los sistemas de escritura del mundo. Antes de Unicode, no era posible tener archivos de texto plano que contuvieran texto en diferentes idiomas ya que se utilizaban codificaciones distintas para diferentes idiomas y cada archivo de texto utiliza una sola codificación. 

Cada caracter Unicode tiene un identificador numérico único denominado code point (‘punto de código’). Hay más de 100000 caracteres Unicode definidos, con puntos de código que van desde 0x0 hasta 0x10FFFF (este último se define en Golang como la constante `unicode.MaxRune`). En la documentación de Unicode, los puntos de código son escritos utilizando cuatro o más dígitos hexadecimales en la forma U+hhhh, por ejemplo, U+00F1 para el caracter ñ.

Cualquier archivo de texto Unicode, ya sea almacenado en disco o en memoria, debe ser representado mediante una codificación. A diferencia de ASCII, Unicode no se puede representar únicamente con un código de 7 u 8 bits, ya que Unicode define más de 256 caracteres. El estándar Unicode define varios Unicode Transformation Formats (encoding), tales como UTF-8, UTF-16 y UTF-32.

UTF-8 (8-bit Unicode Transformation Format) es un formato de codificación de caracteres Unicode que utiliza símbolos de longitud variable (de 1 a 4 bytes para representar cada caracter Unicode--code point). Un formato de codificación es un algoritmo que traduce una lista de números a código binario para su almacenamiento en disco o memoria. Para cadenas de caracteres (strings) que contienen sólo caracteres ASCII de 7-bit (US-ASCII), hay una relación uno a uno entre bytes y caracteres debido a que cada caracter ASCII de 7-bits es representado por un único byte (del mismo valor) en UTF-8. Una consecuencia de esto es que UTF-8 almacena texto en inglés de manera compacta (1 byte por caracter); otra consecuencia es que un archivo de texto codificado mediante ASCII 7-bit es indistinguible de un archivo de texto codificado en UTF-8. UTF-8 es el esquema de codificación más extendido; es la codificación estándar para archivos de texto y la codificación por defecto para archivos XML y JSON.

UTF-8 también es considerado "ASCII Extendido" al preservar el significado del conjunto de códigos de caracteres original ASCII de 7-bit.



### Codificación en Golang

En Go, un code point Unicode es representado por un `rune` en memoria (el tipo `rune` es un sinónimo para `ìnt32`).

Go utiliza el conjunto de caracteres Unicode mediante codificación UTF-8 para cadena de caracteres (`strings`). Un `string` Go es una secuencia inmutable de caracteres de longitud variable donde cada caracter Unicode es representado por uno o más bytes mediante codificación UTF-8.

Un `string` equivale a un `slice de bytes` (`[]byte`), por lo que soporta iteracción byte por byte, sin embargo, los caracteres individuales en el `string` pueden ser directamente indexados únicamente si el `string` contiene caracteres ASCII de 7 bits (ya que la iteracción se realiza byte por byte y los caracteres ASCII se representan con un único byte en UTF-8). Por ejemplo, la letra `ñ` se representa con dos bytes en UTF-8, por lo que si iteramos sobre un string que contiene una `ñ`, obtendremos dos bytes separados que representan cualquier otro caracter, en vez de la `ñ`, que es lo que debería representar.

 Para poder indexar directamente, con independencia de los caracteres que forman el `string`, se puede convertir un `string` Go en un `slice` de code points Unicode (de tipo `[]rune`). 

```go
package main

import (
        "fmt"
)

func main() {
        // String UTF-8
        cadena := "Test de español e ingles"
	cadena2 := []byte(cadena)
        // Obtener el número de bytes en el string
        fmt.Printf("Número de bytes en string: %d\n", len(cadena))
        // Obtener el número de caracteres en el string
        fmt.Printf("Número de caracteres en string: %d\n", len([]rune(cadena)))
        // Indexación de un string
        fmt.Printf("Indexación 12: %c\n", cadena[12])         // Las posiciones 12 y 13 son los bytes que componen el caracter ñ, si se
        fmt.Printf("Indexación 13: %c\n", cadena[13])         // indexan por separado, el valor no sería el correcto.
        fmt.Printf("Indexación 13: %U\n", cadena[13])	      // Se muestra el code point Unicode de la posición 13 que es el caracter representado e impreso por %c
        fmt.Printf("Indexación 13: %c\n", []rune(cadena)[12]) // Para indexar un caracter no ASCII es necesario convertir a un slice de rune ([]rune)
        fmt.Printf("Indexación 14: %c\n", cadena[14])         // La posición 14 corresponde a un caracter ASCII y tiene una relación uno a uno entre caracter y byte.
	// Un for..range convierte cada caracter del string en un code point Unicode en cada iteración.
        fmt.Println("/nfor..range en un string")
        for _, value := range cadena {
                fmt.Printf("%c", value)
        }
        // Un for..range convierte cada caracter del []byte en un byte en cada iteración
        fmt.Println("/n/nfor..range en un array de bytes ([]byte)")
        for _, value := range cadena2 {
                fmt.Printf("%c", value)
        }
}
```

	Número de bytes en string: 25
	Número de caracteres en string: 24
	Indexación 12: Ã
	Indexación 13: ±
	Indexación 12: U+00C3
	Indexación 13: ñ
	Indexación 14: o
	
	for..range en un string
	Test de español e ingles
	
	for..range en un array de bytes ([]byte)
	Test de espaÃ±ol e ingles	

## Enlaces

- [Indice](preface.md)
- Siguiente sección: [URIs](02.0.md)
